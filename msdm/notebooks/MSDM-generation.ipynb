{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b33d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "SAMPLE_RATE = 22050 # < IMPORTANT: do not change\n",
    "STEMS = [\"bass\",\"drums\",\"guitar\",\"piano\"] # < IMPORTANT: do not change\n",
    "ROOT_PATH = Path(\"..\").resolve().absolute()\n",
    "CKPT_PATH = ROOT_PATH / \"ckpts\"\n",
    "DATA_PATH = ROOT_PATH / \"data\"\n",
    "\n",
    "sys.path.append(str(ROOT_PATH))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeebcf-43f4-4b79-9c96-a944b9ced6af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f06d5-5345-43be-a7f9-ae230c1e22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "def load_track(track_folder: Path, stems: List[str]):\n",
    "    wavs = []\n",
    "    for s in stems:\n",
    "        wav, wav_sr = torchaudio.load(track_folder/f\"{s}.wav\")\n",
    "        assert wav_sr == SAMPLE_RATE\n",
    "        assert wav.shape[0] == 1 # < single channel\n",
    "        wavs += [wav]\n",
    "    return torch.cat(wavs, dim=0).unsqueeze(0)\n",
    "\n",
    "def to_audio_widget(wav: torch.Tensor, normalize: bool = False):\n",
    "    assert len(wav.shape) == 2, f\"shape: {wav.shape}\"\n",
    "    return Audio(\n",
    "            wav.sum(dim=0, keepdims=True).cpu(), \n",
    "            rate=SAMPLE_RATE, \n",
    "            normalize=normalize,\n",
    "        )\n",
    "    \n",
    "def wrap_in_out(*obj):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "        display(*obj)\n",
    "    return out\n",
    "\n",
    "def grid_widget(grid_of_objs):\n",
    "    col_boxes = []\n",
    "    for row_of_objs in grid_of_objs:\n",
    "        row_outs = []\n",
    "        for obj in row_of_objs:\n",
    "            row_outs += [obj]\n",
    "        col_boxes += [widgets.HBox(row_outs)]\n",
    "    return widgets.VBox(col_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from audio_diffusion_pytorch import KarrasSchedule\n",
    "from main.separation import MSDMSeparator\n",
    "\n",
    "\n",
    "def score_differential(x, sigma, denoise_fn):\n",
    "    d = (x - denoise_fn(x, sigma=sigma)) / sigma \n",
    "    return d\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_track(\n",
    "    denoise_fn: Callable,\n",
    "    sigmas: torch.Tensor,\n",
    "    noises: torch.Tensor,\n",
    "    source: Optional[torch.Tensor] = None,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    num_resamples: int = 1,\n",
    "    s_churn: float = 0.0,\n",
    "    differential_fn: Callable = score_differential,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    x = sigmas[0] * noises\n",
    "    _, num_sources, _  = x.shape    \n",
    "\n",
    "    # Initialize default values\n",
    "    source = torch.zeros_like(x) if source is None else source\n",
    "    mask = torch.zeros_like(x) if mask is None else mask\n",
    "    \n",
    "    sigmas = sigmas.to(x.device)\n",
    "    gamma = min(s_churn / (len(sigmas) - 1), 2**0.5 - 1)\n",
    "    \n",
    "    # Iterate over all timesteps\n",
    "    for i in tqdm.tqdm(range(len(sigmas) - 1)):\n",
    "        sigma, sigma_next = sigmas[i], sigmas[i+1]\n",
    "\n",
    "        # Noise source to current noise level\n",
    "        noisy_source = source + sigma*torch.randn_like(source)\n",
    "        \n",
    "        for r in range(num_resamples):\n",
    "            # Merge noisy source and current x\n",
    "            x = mask*noisy_source + (1.0 - mask)*x \n",
    "\n",
    "            # Inject randomness\n",
    "            sigma_hat = sigma * (gamma + 1)            \n",
    "            x_hat = x + torch.randn_like(x) * (sigma_hat**2 - sigma**2)**0.5\n",
    "\n",
    "            # Compute conditioned derivative\n",
    "            d = differential_fn(x=x_hat, sigma=sigma_hat, denoise_fn=denoise_fn)\n",
    "\n",
    "            # Update integral\n",
    "            x = x_hat + d*(sigma_next - sigma_hat)\n",
    "                \n",
    "            # Renoise if not last resample step\n",
    "            if r < num_resamples - 1:\n",
    "                x = x + torch.randn_like(x) * (sigma**2 - sigma_next**2)**0.5\n",
    "\n",
    "    return mask*source + (1.0 - mask)*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf83c1-9d89-4ecc-99ea-4d1461c228af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446654e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.module_base import Model\n",
    "\n",
    "# Load model\n",
    "model = Model.load_from_checkpoint(CKPT_PATH / f\"glorious-star-335/epoch=729-valid_loss=0.014.ckpt\").to(DEVICE)\n",
    "denoise_fn = model.model.diffusion.denoise_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa911e8c-b943-41f6-9416-7e119e699c5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation hyper-parameters\n",
    "s_churn = 20.0\n",
    "batch_size = 9\n",
    "num_steps = 150\n",
    "num_resamples = 1\n",
    "\n",
    "# Define timestep schedule\n",
    "schedule = KarrasSchedule(sigma_min=1e-4, sigma_max=20.0, rho=7)(num_steps, DEVICE)\n",
    "\n",
    "# Unconditionally sample from diffusion model\n",
    "generated_tracks = generate_track(\n",
    "    denoise_fn,\n",
    "    sigmas=schedule,\n",
    "    noises=torch.randn(batch_size, 4, 262144).to(DEVICE),\n",
    "    s_churn=s_churn,\n",
    "    num_resamples=num_resamples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8fcb3-8056-4e83-9606-b03dad22299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, HTML, Markdown\n",
    "\n",
    "num_generations = generated_tracks.shape[0]\n",
    "w, h = 3, num_generations//3 + int(num_generations%3 > 0)\n",
    "\n",
    "# Organize results into a grid\n",
    "grid = []\n",
    "for i in range(h):\n",
    "    row = []\n",
    "    for j in range(w):\n",
    "        index = i*w + j\n",
    "        if index >= num_generations:\n",
    "            continue \n",
    "\n",
    "        row.append(\n",
    "            wrap_in_out(\n",
    "                Markdown(f\"**Sample at index** [{i*w+j}]:\"),\n",
    "                to_audio_widget(generated_tracks[index,:,:]),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    grid.append(row)\n",
    "\n",
    "        \n",
    "# Show results\n",
    "display(Markdown(\"## **Generations:**\"))\n",
    "display(grid_widget(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06ce95-8894-493b-9177-4ac69dc4f368",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Partial Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec74af-6420-423b-be49-a09d67284487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio track. Shape = [1, num_sources, num_samples]\n",
    "sources = load_track(DATA_PATH / \"dummy_slakh2100/test/Track01888\", STEMS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7f19d-f93e-4f44-9851-fafd2be6d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to the track sources\n",
    "for i,s in enumerate(stems):\n",
    "    print(f\"{s}:\")\n",
    "    display(Audio(sources[:,i,:], rate=SAMPLE_RATE, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a507dc-c2d6-48c2-97c0-850df4bd7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial generation hyper-parameters\n",
    "s_churn = 10.0\n",
    "batch_size = 4\n",
    "num_resamples = 1\n",
    "num_steps = 256\n",
    "start_second = 20.0\n",
    "stems_to_inpaint = {\"bass\", \"guitar\", \"piano\"}\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_rms(source_waveforms): \n",
    "    # Get Root Mean Square of waveforms\n",
    "    return torch.mean(source_waveforms ** 2, dim=-1)**0.5\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_inpaint_mask(sources, stem_to_inpaint: List[int]):\n",
    "    mask = torch.ones_like(sources)\n",
    "    for stem_idx in stem_to_inpaint:\n",
    "        mask[:,stem_idx,:] = 0.0\n",
    "    return mask\n",
    "        \n",
    "# Select window from track\n",
    "start_sample = int(start_second*SAMPLE_RATE)\n",
    "source_chunk = sources[:,:, start_sample:start_sample + 262144] \n",
    "\n",
    "# Generate inpainting mask\n",
    "assert len([s for s in stems_to_inpaint if s not in STEMS]) == 0 # < stems_to_inpaint must be a subset of STEMS\n",
    "stemidx_to_inpaint = [i for i,s in enumerate(STEMS) if s in stems_to_inpaint]\n",
    "inpaint_mask = generate_inpaint_mask(source_chunk, stem_to_inpaint=stemidx_to_inpaint)\n",
    "\n",
    "# Define timestep schedule\n",
    "schedule = KarrasSchedule(sigma_min=1e-4, sigma_max=20.0, rho=7)(num_steps, DEVICE)\n",
    "\n",
    "# Inpaint tracks together with the original sources\n",
    "inpainted_tracks = generate_track(\n",
    "    source=source_chunk,\n",
    "    mask=inpaint_mask,\n",
    "    denoise_fn=denoise_fn,\n",
    "    sigmas=schedule,\n",
    "    noises=torch.randn_like(source_chunk).repeat(batch_size, 1, 1),\n",
    "    s_churn=s_churn,\n",
    "    num_resamples=num_resamples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655829a-837d-4399-aada-fac7b0e1bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, HTML, Markdown\n",
    "\n",
    "num_inpaints = inpainted_tracks.shape[0]\n",
    "mask = inpaint_mask.squeeze(0)\n",
    "\n",
    "# Organize results into a grid\n",
    "grid = []\n",
    "for i in range(num_inpaints):\n",
    "    wav = inpainted_tracks[i,:,:]\n",
    "    inpaint_widget = to_audio_widget((1.0 - mask)*wav)\n",
    "    mix_widget = to_audio_widget(wav)\n",
    "\n",
    "    row = [\n",
    "        wrap_in_out(Markdown(f\"**Inpainted + Original track** [{i}]:\"), mix_widget),\n",
    "        wrap_in_out(Markdown(f\"**Inpainted track** [{i}]:\"), inpaint_widget),\n",
    "    ]\n",
    "    grid.append(row)\n",
    "        \n",
    "# Show results\n",
    "display(Markdown(\"## **Inpainting results:**\"))\n",
    "display(wrap_in_out(Markdown(f\"**Original track**:\"), to_audio_widget(mask*source_chunk.squeeze(0))))\n",
    "display(grid_widget(grid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
